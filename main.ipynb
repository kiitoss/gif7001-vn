{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e352419",
   "metadata": {},
   "source": [
    "# Reconnaissance de caractères et de symboles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5315bb6e",
   "metadata": {},
   "source": [
    "## Détection de texte dans une image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a19a401",
   "metadata": {},
   "source": [
    "Pour détecter le texte au sein d'une image, nous utilisons le plugin python **pytesseract**.\n",
    "\n",
    "Dans un premier temps, nous effectuons des pré-traitements sur l'image (suppression de la couleur, dilatation, erosion pour réduire le bruit...) afin de la simplifier et d'accélerer le traitement.\n",
    "\n",
    "\n",
    "Ensuite, nous extrayons (grâce à pytesseract) les zones contenant du texte puis, en bouclant sur ces zones, nous pouvons extraire les coordonnées et le contenu de ces dernières pour les stocker dans une liste réutilisable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "70f42fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import pytesseract\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import Image, display\n",
    "import threading\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2864a90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_text(frame):\n",
    "    img = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    kernel = np.ones((1, 1), np.uint8)\n",
    "    img = cv2.dilate(img, kernel, iterations=10)\n",
    "    img = cv2.erode(img, kernel, iterations=10)\n",
    "    \n",
    "    hImg, wImg = img.shape[:2]\n",
    "    boxes = pytesseract.image_to_data(img)\n",
    "\n",
    "    data = []\n",
    "    \n",
    "    for x, b in enumerate(boxes.splitlines( )):\n",
    "        if x != 0:\n",
    "            b = b.split( )\n",
    "            if len(b)==12:\n",
    "                x, y, w, h = int(b[6]), int(b[7]), int(b[8]), int(b[9])\n",
    "                content = b[11]\n",
    "                data.append((x, y, w, h, content))\n",
    "                \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a309be8",
   "metadata": {},
   "source": [
    "## Détection de QR codes et de codes barres dans une image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c88032",
   "metadata": {},
   "source": [
    "Comme pour la détection du texte, les détections de codes barres et de codes QR peuvent être effectuées à l'aide d'un plugin python : **pyzbar**.\n",
    "\n",
    "A partir d'une image légèrement prétraitée, nous pouvons utiliser la fonction **pyzbar.decode(...)** pour extraire les différents codes présents. Cette fonction est très pratique, et nous avons ajouté un comportement différent dans le cas où l'image (provenant d'une webcam par exemple) serait inversée. A ce moment là, nous allons décaler dans le sens opposé les zones détectées sur l'image par rapport au milieu de l'axe X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "7c322f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from pyzbar import pyzbar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9e2fda29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_image(img, frame_reversed=False):\n",
    "    codes = pyzbar.decode(img)\n",
    "    \n",
    "    if frame_reversed:\n",
    "        img_width = img.shape[1]\n",
    "        reversed_codes = []\n",
    "\n",
    "        for code in codes:\n",
    "            reversed_x = img_width - code.rect.left - (code.rect.width / 2)\n",
    "\n",
    "            reversed_rect = Rect(\n",
    "                left=int(reversed_x - code.rect.width / 2),\n",
    "                top=code.rect.top,\n",
    "                width=code.rect.width,\n",
    "                height=code.rect.height\n",
    "            )\n",
    "\n",
    "            reversed_code = Decoded(\n",
    "                data=code.data,\n",
    "                type=code.type,\n",
    "                rect=reversed_rect,\n",
    "                polygon=code.polygon,\n",
    "                quality=code.quality,\n",
    "                orientation=code.orientation\n",
    "            )\n",
    "\n",
    "            reversed_codes.append(reversed_code)\n",
    "\n",
    "        return reversed_codes\n",
    "    \n",
    "    return codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "58e930d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_image(img, code):\n",
    "    (x, y, w, h) = code.rect\n",
    "    cv2.rectangle(img, (x, y), (x + w, y + h), (0, 0, 255), 2)\n",
    "\n",
    "    codeData = code.data.decode(\"utf-8\")\n",
    "    codeType = code.type\n",
    "\n",
    "    text = \"{} ({})\".format(codeData, codeType)\n",
    "    cv2.putText(img, text, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)\n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ad8f0d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_and_update_image(original):\n",
    "    if original is None:\n",
    "        return [], original, original\n",
    "    \n",
    "    updated = original.copy()\n",
    "    img = cv2.cvtColor(original, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    codes = decode_image(img)\n",
    "    for code in codes:\n",
    "        updated = update_image(updated, code)\n",
    "\n",
    "    return codes, original, updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "bdf170ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_and_update_image_from_path(img_path):\n",
    "    original = cv2.imread(img_path)\n",
    "\n",
    "    return decode_and_update_image(original)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4a1cea",
   "metadata": {},
   "source": [
    "## Détection de logos dans une image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267c5daa",
   "metadata": {},
   "source": [
    "La détection de logo est certainement la détection la plus compliquée du projet. Les ressources demandées sont très importantes alors il est nécessaire, en plus du pré-traitement de l'image, de la réduire en taille.\n",
    "\n",
    "Pour comparer les frames à un ensemble de logos, nous allons utiliser l'algorithme d'intelligence artificielle **KNN (k nearest neighbor)** qui nous permet, à partir de la représentation d'un logo de trouver s'il est présent dans l'image.\n",
    "\n",
    "Au départ, nous devons donc récupérer les **flanns** de tous les logos (ici ceux qui sont présents dans le dossier 'data/images/logos/src'). Les flanns sont les descripteurs du logo qui prennent en comptes les points-clés de l'image. Une fois ces flanns récupérés, nous pourrons les utiliser sur chaque frame pour tester la présence des logos grâce à la fonction **flann.knnMatch(...)**. \n",
    "\n",
    "Si la présence d'un logo est notée, il nous est ensuite possible de le retrouver la modification de perspective de ce logo au sein de notre frame grâce à la fonction **findHomography(...)** de **opencv**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "222f6205",
   "metadata": {},
   "outputs": [],
   "source": [
    "SIFT_SIGMA = 1.6\n",
    "SIFT_INIT_SIGMA = 0.5\n",
    "FX_FY = 0.7 # 0.5 = fast, 2 = slow\n",
    "\n",
    "def resize_image(img, fx_fy):\n",
    "    img = cv2.resize(img, (0, 0), fx=fx_fy, fy=fx_fy, interpolation=cv2.INTER_LINEAR)\n",
    "    return img\n",
    "\n",
    "def create_initial_image(img, sigma=SIFT_SIGMA):\n",
    "    if len(img.shape) == 3 and img.shape[-1] == 3:\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "    sigma_diff = math.sqrt(max(sigma ** 2 - (2* SIFT_INIT_SIGMA) ** 2, 0.01))\n",
    "\n",
    "    cv2.GaussianBlur(img, (0, 0), sigmaX=sigma_diff, sigmaY=sigma_diff)\n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "4bdaf26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_flanns(sift, logos_path):\n",
    "    flanns = []\n",
    "    \n",
    "    index_logo = 1\n",
    "    for img_path in os.listdir(logos_path):\n",
    "        t1 = time.time()\n",
    "        \n",
    "        full_path = logos_path + \"/\" + img_path\n",
    "        \n",
    "        initial_logo = cv2.imread(full_path, cv2.IMREAD_GRAYSCALE)\n",
    "        \n",
    "        logo = create_initial_image(initial_logo)\n",
    "        \n",
    "        # find the keypoints and descriptors with SIFT \n",
    "        kp_image, desc_image = sift.detectAndCompute(logo, None) \n",
    "\n",
    "        # initializing the dictionary \n",
    "        index_params = dict(algorithm = 0, trees = 5) \n",
    "        search_params = dict() \n",
    "\n",
    "        # by using Flann Matcher \n",
    "        flann = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "        \n",
    "        flanns.append((logo, kp_image, desc_image, flann))\n",
    "        \n",
    "        t2 = time.time()\n",
    "        \n",
    "        print(f'Récupération logo {index_logo} : {t2 - t1}')\n",
    "        index_logo += 1\n",
    "    \n",
    "    return flanns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "9b0c613b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_logo_data(logos_id2name, logo_id, sift, flann, logo, desc_image, kp_image, original_image, scale_percent, distance_threshold=0.6):    \n",
    "    resized_image = resize_image(original_image, scale_percent/100)\n",
    "    image = create_initial_image(resized_image)\n",
    "        \n",
    "    # Find keypoints and descriptors in the current frame\n",
    "    kp_grayframe, desc_grayframe = sift.detectAndCompute(image, None)\n",
    "\n",
    "    # Find matches using FLANN\n",
    "    matches = flann.knnMatch(desc_image, desc_grayframe, k=2)\n",
    "\n",
    "    # Filter good points based on distance\n",
    "    good_points = [m for m, n in matches if m.distance < distance_threshold * n.distance]\n",
    "\n",
    "    if len(good_points) < 4:\n",
    "        return None\n",
    "\n",
    "    # Get corresponding points for perspective transformation\n",
    "    query_pts = np.float32([kp_image[m.queryIdx].pt for m in good_points]).reshape(-1, 1, 2)\n",
    "    train_pts = np.float32([kp_grayframe[m.trainIdx].pt for m in good_points]).reshape(-1, 1, 2)\n",
    "\n",
    "    # Find perspective transformation\n",
    "    matrix, mask = cv2.findHomography(query_pts, train_pts, cv2.RANSAC, 5.0, confidence=0.997)\n",
    "\n",
    "    if matrix is None:\n",
    "        return None\n",
    "    \n",
    "    # Apply perspective transformation to logo corners\n",
    "    h, w = logo.shape\n",
    "    pts = np.float32([[0, 0], [0, h], [w, h], [w, 0]]).reshape(-1, 1, 2)\n",
    "    \n",
    "    dst = cv2.perspectiveTransform(pts, matrix)\n",
    "\n",
    "    dst /= scale_percent / 100.0\n",
    "    \n",
    "    return {\n",
    "        'id': logo_id,\n",
    "        'data': dst\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "21a60277",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_logos_data(flanns, image, scale_percent):\n",
    "    logos_data = []\n",
    "    for index, (logo, kp_image, desc_image, flann) in enumerate(flanns):\n",
    "        logos_data.append(find_logo_data(logos_id2name, index, sift, flann, logo, desc_image, kp_image, image, scale_percent))\n",
    "    return logos_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4ecf66",
   "metadata": {},
   "source": [
    "## Détection de texte, QR codes, codes barres et logos dans une image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87aaca91",
   "metadata": {},
   "source": [
    "Après avoir testé individuellement chacune de nos fonctions précédentes, nous pouvons fusionner le tout dans une seule grande partie qui s'occupe de récupérer les images de la webcam, analyser le texte, les codes et les logos, puis d'écrire les informations importantes directement sur l'image avant de l'afficher à l'écran.\n",
    "\n",
    "On pourra noter ici l'instruction suivante dans la boucle de la fonction `view` :\n",
    "```py\n",
    "if index_frame % 12 == 0:\n",
    "    data = find_text(original_frame)\n",
    "```\n",
    "qui nous permet de ne rechercher le texte approximativement une fois toutes les demi-secondes pour éviter la surcharge de travail de pytesseract.\n",
    "\n",
    "De plus, nous pourrons noter ici dans la fonction `add_text_to_frame` :\n",
    "```py\n",
    "if len(content) < 5:\n",
    "    continue\n",
    "```\n",
    "Cette instruction nous permet de n'afficher que les mots détectés d'une taille supérieure à 5. Cela évite d'afficher trop de mots à l'écran et se focalise sur les mots les plus longs que l'on pourrait supposer comme possédant une plus grande signification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "32d0586d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "\n",
    "import pytesseract\n",
    "from pyzbar import pyzbar\n",
    "from pyzbar.pyzbar import Point, Rect, Decoded\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import Image, display\n",
    "import threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "10f80b00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'la-parisienne', 1: 'lissac', 2: 'folio-essais', 3: 'traditional-medicinals'}\n",
      "Récupération logo 1 : 0.026321887969970703\n",
      "Récupération logo 2 : 0.02335524559020996\n",
      "Récupération logo 3 : 0.02161407470703125\n",
      "Récupération logo 4 : 0.01456308364868164\n",
      "Flanns récupérés (0.08712172508239746s)\n"
     ]
    }
   ],
   "source": [
    "LOGOS_PATH = './data/images/logos/sources'\n",
    "OUTPUT_PATH = './data/outputs'\n",
    "\n",
    "sift = cv2.SIFT_create()\n",
    "\n",
    "logos_id2name = {}\n",
    "for index, img_path in enumerate(os.listdir(LOGOS_PATH)):\n",
    "    logos_id2name[index] = img_path.split('.')[0]\n",
    "print(logos_id2name)\n",
    "\n",
    "t1 = time.time()\n",
    "flanns = get_flanns(sift, LOGOS_PATH)\n",
    "t2 = time.time()\n",
    "print(f\"Flanns récupérés ({t2 - t1}s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "53f85522",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_text_to_frame(data, frame):\n",
    "    for (x, y, w, h, content) in data:\n",
    "        if len(content) < 5:\n",
    "            continue\n",
    "        cv2.rectangle(frame, (x, y), (w+x, h+y), (0, 255, 0), 1)\n",
    "        cv2.putText(frame, content, (x, y), cv2.FONT_HERSHEY_COMPLEX, 0.5, (0, 255, 0), 2)\n",
    "    \n",
    "    return frame\n",
    "\n",
    "def add_codes_to_frame(codes, frame):\n",
    "    for code in codes:\n",
    "        frame = update_image(frame, code)\n",
    "    return frame\n",
    "\n",
    "def add_logos_to_frame(logos_data, frame):\n",
    "    if logos_data is None:\n",
    "        return frame\n",
    "    \n",
    "    for logo_data in logos_data:\n",
    "        if logo_data is None:\n",
    "            continue\n",
    "            \n",
    "        logo_name = logos_id2name.get(logo_data['id'], \"Unknown\")\n",
    "        data = logo_data['data']\n",
    "        \n",
    "        # Draw the perspective-transformed logo on the frame\n",
    "        homography = cv2.polylines(frame, [np.int32(data)], True, (255, 0, 0), 3)\n",
    "\n",
    "        # Write the name of the logo on the image\n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        font_scale = 0.5\n",
    "        font_thickness = 1\n",
    "        text_size = cv2.getTextSize(logo_name, font, font_scale, font_thickness)[0]\n",
    "        text_position = (int(data[0, 0, 0] - text_size[0] / 2), int(data[0, 0, 1] - 5))\n",
    "        cv2.putText(homography, logo_name, text_position, font, font_scale, (255, 0, 0), font_thickness, cv2.LINE_AA)\n",
    "    \n",
    "    return frame\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "7cad7397",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Widget Javascript not detected.  It may not be installed or enabled properly. Reconnecting the current kernel may help.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73f0fd272ad948ee84c216aaa0e293b9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenCV: FFMPEG: tag 0x47504a4d/'MJPG' is not supported with codec id 7 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n",
      "Exception in thread Thread-50 (view):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/threading.py\", line 953, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/tmp/ipykernel_24228/168502909.py\", line 43, in view\n",
      "  File \"/tmp/ipykernel_24228/463870029.py\", line 2, in decode_image\n",
      "  File \"/home/antoine/.local/lib/python3.10/site-packages/pyzbar/pyzbar.py\", line 207, in decode\n",
      "    pixels, width, height = _pixel_data(image)\n",
      "  File \"/home/antoine/.local/lib/python3.10/site-packages/pyzbar/pyzbar.py\", line 173, in _pixel_data\n",
      "    pixels, width, height = image\n",
      "TypeError: cannot unpack non-iterable NoneType object\n"
     ]
    }
   ],
   "source": [
    "stopButton = widgets.ToggleButton(\n",
    "    value=False,\n",
    "    description='Stop',\n",
    "    disabled=False,\n",
    "    button_style='danger',\n",
    "    tooltip='Description',\n",
    "    icon='square'\n",
    ")\n",
    "\n",
    "\n",
    "def view(button):\n",
    "    previous_logo_id = -1\n",
    "    previous_codes = None\n",
    "    \n",
    "    cap = cv2.VideoCapture(0)\n",
    "    display_handle=display(None, display_id=True)\n",
    "    \n",
    "    index_frame = 0\n",
    "    data = []\n",
    "    \n",
    "    \n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'MJPG')\n",
    "    videoWriter = cv2.VideoWriter(OUTPUT_PATH + '/presentation.mp4', fourcc, 8.0, (640,480))\n",
    "\n",
    "    \n",
    "    while True:\n",
    "        index_frame += 1\n",
    "            \n",
    "        if stopButton.value==True:\n",
    "            cap.release()\n",
    "            videoWriter.release()\n",
    "            display_handle.update(None)\n",
    "        \n",
    "        _, original_frame = cap.read()\n",
    "            \n",
    "       \n",
    "        if index_frame % 12 == 0:\n",
    "            data = find_text(original_frame)\n",
    "        \n",
    "        \n",
    "        barcode_frame = cv2.flip(original_frame, 1) # camera reverses image\n",
    "        codes = decode_image(barcode_frame, frame_reversed=True)\n",
    "        logos_data = get_logos_data(flanns, original_frame, scale_percent=70)\n",
    "        \n",
    "            \n",
    "        # Modify current frame\n",
    "        frame = add_text_to_frame(data, original_frame)\n",
    "        frame = add_codes_to_frame(codes, frame)\n",
    "        frame = add_logos_to_frame(logos_data, frame)\n",
    "        \n",
    "        videoWriter.write(frame)\n",
    "            \n",
    "        _, frame = cv2.imencode('.jpeg', frame)\n",
    "        \n",
    "        \n",
    "        display_handle.update(Image(data=frame.tobytes()))\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "display(stopButton)\n",
    "thread = threading.Thread(target=view, args=(stopButton,))\n",
    "thread.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d61f0d7",
   "metadata": {},
   "source": [
    "Aujourd'hui, de plus en plus d'outils nous permettent d'extraire des informations des vidéos de façon extrêmement rapide. Le pré-traitement et la réduction d'échelle d'une image sont essentiels pour une analyse efficace en temps réel.\n",
    "\n",
    "Notre projet peut être perçu comme un **POC (*proof-of-concept*)**. Il reste des corrections à apporter, notamment sur la détection de textes ou sur la détection de logos qui reste délicate quand un logo entre ou sort de l'écran. Nous pourrions adopter des comportements différents quand un logo ou un code-barre est detecté ou nous pourrions même prioriser certaines informations à l'écran quand ce dernier est surchargé."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755d52a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
